import {
	SlashCommandBuilder,
	ChatInputCommandInteraction,
	EmbedBuilder,
} from "discord.js";
import { InferenceClient } from "@huggingface/inference";
import { logger } from "~/logger";

const client = new InferenceClient(Bun.env.HF_TOKEN);

function splitIntoChunks(text: string, maxLength = 1024): string[] {
	const chunks: string[] = [];
	let current = "";

	for (const line of text.split("\n")) {
		if ((current + line + "\n").length > maxLength) {
			chunks.push(current);
			current = "";
		}
		current += line + "\n";
	}

	if (current.trim()) {
		chunks.push(current);
	}

	return chunks;
}

export const data = new SlashCommandBuilder()
	.setName("chat")
	.setDescription("Send a message to the Llama 3.1 model")
	.addStringOption((option) =>
		option
			.setName("message")
			.setDescription("What you want to say to the model")
			.setRequired(true),
	);

export async function execute(interaction: ChatInputCommandInteraction) {
	const userMessage = interaction.options.getString("message", true);
	await interaction.deferReply();

	try {
		let visibleText = "";
		let afterThink = false;
		let rawBuffer = "";
		let lastEditTime = Date.now();

		const stream = await client.chatCompletionStream({
			model: "deepseek-ai/DeepSeek-R1-0528",
			provider: "sambanova",
			messages: [
				{ role: "user", content: userMessage },
			],
			temperature: 0.5,
			stream: true,
		});

		// Initial embed while we wait
		const embed = new EmbedBuilder()
			.setTitle(`User has asked: ${userMessage}`)
			.setDescription("`Thinking...`")
			.setColor(0x5865F2)

		await interaction.editReply({ embeds: [embed] });

		for await (const chunk of stream) {
			const content = chunk?.choices?.[0]?.delta?.content;
			if (!content) continue;

			rawBuffer += content;

			// Once </think> is seen, begin rendering
			if (!afterThink) {
				const endTagIndex = rawBuffer.lastIndexOf("</think>");
				if (endTagIndex !== -1) {
					afterThink = true;
					visibleText = rawBuffer.slice(endTagIndex + "</think>".length);
				}
			} else {
				visibleText += content;
			}

			const now = Date.now();
			if (now - lastEditTime > 500) {
				const partialEmbed = new EmbedBuilder()
					.setTitle(`You asked: ${userMessage}`)
					.setDescription(visibleText.slice(0, 4096 - 5) + " ‚ñå")
					.setColor(0x5865F2)
					.setFooter({ text: `You: ${userMessage}` });

				await interaction.editReply({ embeds: [partialEmbed] });
				lastEditTime = now;
			}
		}

		const finalOutput = visibleText.trim();
		if (!finalOutput) {
			const failEmbed = new EmbedBuilder()
				.setTitle(`You asked: ${userMessage}`)
				.setDescription("_[No meaningful response]_ ü§∑üèΩ‚Äç‚ôÄÔ∏è")
				.setColor(0xFF5555)
				.setFooter({ text: `You: ${userMessage}` });

			await interaction.editReply({ embeds: [failEmbed] });
			return;
		}

		// Split across multiple embeds if needed
		const chunks = splitIntoChunks(finalOutput, 1024);
		const firstEmbed = new EmbedBuilder()
			.setTitle(`You: ${userMessage}`)
			.setColor(0x5865F2)
			.setDescription(chunks[0] as any)

		await interaction.editReply({ embeds: [firstEmbed] });

		for (let i = 1; i < chunks.length; i++) {
			const embed = new EmbedBuilder()
				.setColor(0x5865F2)
				.setDescription(chunks[i] as any);
			await interaction.followUp({ embeds: [embed] });
		}
	} catch (err) {
		logger.error("Streaming inference error:", err);
		const errorEmbed = new EmbedBuilder()
			.setTitle(":Error:")
			.setDescription(`Something went wrong:\n\`\`\`${String((err as any)?.message || err)}\`\`\``)
			.setColor(0xFF0000)
			.setFooter({ text: `You asked: ${userMessage}` });

		await interaction.editReply({ embeds: [errorEmbed] });
	}
}

